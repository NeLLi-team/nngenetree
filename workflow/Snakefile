##############################################
# workflow/Snakefile
##############################################
import os
import glob
# Get absolute paths to workflow directory and scripts folder
WORKFLOW_DIR = workflow.basedir
SCRIPTS_DIR = os.path.join(WORKFLOW_DIR, "scripts")

# Set pipeline version
VERSION = "1.0.0"

# Configuration
configfile: WORKFLOW_DIR + "/config.txt"

INPUT_DIR = config.get("input_dir", "example")
# Strip any quotes and whitespace that might have been passed in the config
INPUT_DIR = INPUT_DIR.strip().strip('"').strip("'").strip() if isinstance(INPUT_DIR, str) else INPUT_DIR

BLAST_DB = config.get("blast_db", "/clusterfs/jgi/scratch/science/mgs/nelli/databases/nr/nr")
BLAST_HITS = config.get("blast_hits", 50)
CLOSEST_NEIGHBORS = config.get("closest_neighbors", 10)  # Number of closest neighbors to extract per query
QUERY_FILTER = config.get("query_filter", "")  # Optional comma-separated list of query prefixes to filter by
ITOL_TAX_LEVEL = config.get("itol_tax_level", "class")  # Taxonomy level for iTOL visualization, default to class

# Check for a valid input directory
if not isinstance(INPUT_DIR, str) or not INPUT_DIR:
    raise ValueError(
        "No valid 'input_dir' was provided. "
        "Please specify --config input_dir=workflow/config.txt or set it in config.txt."
    )

# Resource helper
def get_res(rule_name, key):
    """Helper to fetch resource info from config, e.g. get_res('run_diamond_blastp','threads')."""
    # If you have a 'resources' dict in your config, this returns config["resources"][rule_name][key].
    return config["resources"][rule_name][key]

# Get list of .faa files in the input directory
INPUT_FILES = glob.glob(os.path.join(INPUT_DIR, "*.faa"))
samples = {os.path.splitext(os.path.basename(f))[0]: f for f in INPUT_FILES}
sample_order = list(samples.keys())

# Define the base output directory
# Use environment variable if running in container, otherwise use local path
# Avoid f-string issues by using explicit concatenation
OUTPUT_BASE_DIR = config.get("output_dir", None)
if OUTPUT_BASE_DIR is None:
    OUTPUT_BASE_DIR = INPUT_DIR + "_nngenetree"

if os.path.exists("/data/output") and os.access("/data/output", os.W_OK):
    # We're in a container with write access to /data/output
    OUTPUT_BASE_DIR = "/data/output"

# Mark certain rules as local only
localrules: process_blast_results, check_blast_output, 
            trim_alignment, combine_sequences, decorate_tree, calculate_tree_stats, 
            extract_closest_neighbors, assign_taxonomy, all

# Set the onstart handler to print a message
onstart:
    print("Starting workflow with --rerun-triggers mtime")

rule all:
    input:
        expand(
            os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "decorated_tree.png"),
            sample=samples.keys()
        ),
        expand(
            os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "tree_stats.tab"),
            sample=samples.keys()
        ),
        expand(
            os.path.join(OUTPUT_BASE_DIR, "{sample}", "placement_results.json"),
            sample=samples.keys()
        ),
        os.path.join(OUTPUT_BASE_DIR, "combined_placement_results.json")
    output:
        completion_file = INPUT_DIR + "_nngenetree_completion.log"
    shell:
        """
        echo "🌟 NNGeneTree Pipeline Run Summary 🌟" > {output.completion_file}
        echo "===========================================" >> {output.completion_file}
        echo "📋 Pipeline Version: {VERSION}" >> {output.completion_file}
        echo "📅 Completed at: $(date '+%Y-%m-%d %H:%M:%S')" >> {output.completion_file}
        echo "📁 Input Directory: {INPUT_DIR}" >> {output.completion_file}
        echo "🔍 BLAST Database: {BLAST_DB}" >> {output.completion_file}
        echo "💾 Output Directory: {OUTPUT_BASE_DIR}" >> {output.completion_file}
        echo "===========================================" >> {output.completion_file}
        
        echo -e "\\n📊 Sample Analysis Summary:" >> {output.completion_file}
        echo "===========================================" >> {output.completion_file}
        
        # Process each sample
        for sample in $(ls -d {OUTPUT_BASE_DIR}/*/); do
            sample_name=$(basename $sample)
            echo -e "\\n🧬 Sample: $sample_name" >> {output.completion_file}
            
            # Count BLAST hits
            if [ -f "{OUTPUT_BASE_DIR}/blast_results/$sample_name.m8" ]; then
                hit_count=$(wc -l < {OUTPUT_BASE_DIR}/blast_results/$sample_name.m8)
                echo "   - BLAST Hits: $hit_count" >> {output.completion_file}
            else
                echo "   - BLAST Hits: 0 (no hits found)" >> {output.completion_file}
            fi
            
            # Check if tree was generated
            if [ -f "{OUTPUT_BASE_DIR}/$sample_name/tree/final_tree.treefile" ]; then
                echo "   - 🌳 Tree Generated: Yes" >> {output.completion_file}
                
                # Count unique taxa at different levels from taxonomy file
                if [ -f "{OUTPUT_BASE_DIR}/$sample_name/closest_neighbors_with_taxonomy.csv" ]; then
                    echo "   - 🔬 Taxonomy Analysis:" >> {output.completion_file}
                    
                    # Create a temporary file with taxonomy fields
                    csv_file="{OUTPUT_BASE_DIR}/$sample_name/closest_neighbors_with_taxonomy.csv"
                    cut -d',' -f5 "$csv_file" | tail -n +2 | sort > taxonomy_temp.txt
                    
                    # Extract domains (1st field in taxonomy string)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            echo "$line" | cut -d';' -f1
                        else
                            echo "Unknown"
                        fi
                    done | sort | uniq > domains.txt
                    domain_count=$(wc -l < domains.txt)
                    
                    # Extract higher groups (2nd field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 2 && ! -z "${{fields[1]}}" ]]; then
                                echo "${{fields[1]}}"
                            fi
                        fi
                    done | sort | uniq > higher_groups.txt
                    [ -s higher_groups.txt ] && higher_group_count=$(wc -l < higher_groups.txt) || higher_group_count=0
                    
                    # Extract phyla (3rd field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 3 && ! -z "${{fields[2]}}" ]]; then
                                echo "${{fields[2]}}"
                            fi
                        fi
                    done | sort | uniq > phyla.txt
                    [ -s phyla.txt ] && phylum_count=$(wc -l < phyla.txt) || phylum_count=0
                    
                    # Extract classes (4th field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 4 && ! -z "${{fields[3]}}" ]]; then
                                echo "${{fields[3]}}"
                            fi
                        fi
                    done | sort | uniq > classes.txt
                    [ -s classes.txt ] && class_count=$(wc -l < classes.txt) || class_count=0
                    
                    # Extract orders (5th field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 5 && ! -z "${{fields[4]}}" ]]; then
                                echo "${{fields[4]}}"
                            fi
                        fi
                    done | sort | uniq > orders.txt
                    [ -s orders.txt ] && order_count=$(wc -l < orders.txt) || order_count=0
                    
                    # Extract families (6th field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 6 && ! -z "${{fields[5]}}" ]]; then
                                echo "${{fields[5]}}"
                            fi
                        fi
                    done | sort | uniq > families.txt
                    [ -s families.txt ] && family_count=$(wc -l < families.txt) || family_count=0
                    
                    # Extract genera (7th field)
                    cat taxonomy_temp.txt | while read line; do
                        if [[ "$line" == *";"* ]]; then
                            IFS=';' read -ra fields <<< "$line"
                            if [[ ${{#fields[@]}} -ge 7 && ! -z "${{fields[6]}}" ]]; then
                                echo "${{fields[6]}}"
                            fi
                        fi
                    done | sort | uniq > genera.txt
                    [ -s genera.txt ] && genus_count=$(wc -l < genera.txt) || genus_count=0
                    
                    # Output the counts with actual taxonomic names
                    echo "     • Domains: $domain_count: $(cat domains.txt | tr '\n' ', ' | sed 's/, *$//')" >> {output.completion_file}
                    echo "     • Higher Groups: $higher_group_count: $(cat higher_groups.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    echo "     • Phyla: $phylum_count: $(cat phyla.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    echo "     • Classes: $class_count: $(cat classes.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    echo "     • Orders: $order_count: $(cat orders.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    echo "     • Families: $family_count: $(cat families.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    echo "     • Genera: $genus_count: $(cat genera.txt 2>/dev/null | tr '\n' ', ' | sed 's/, *$//' || echo "")" >> {output.completion_file}
                    
                    # Print top 3 most common domains
                    echo "     • Top domains:" >> {output.completion_file}
                    cat domains.txt | sort | uniq -c | sort -nr | head -3 | while read count domain; do
                        echo "       - $domain: $count" >> {output.completion_file}
                    done
                    
                    # Clean up temporary files
                    rm -f taxonomy_temp.txt domains.txt higher_groups.txt phyla.txt classes.txt orders.txt families.txt genera.txt
                else
                    echo "   - 🔬 Taxonomy Analysis: Not available" >> {output.completion_file}
                fi
            else
                echo "   - 🌳 Tree Generated: No" >> {output.completion_file}
            fi
        done
        
        echo -e "\\n✅ All processes completed successfully ✅" >> {output.completion_file}
        echo "===========================================" >> {output.completion_file}
        """

###################################
# Rule: run_diamond_blastp
###################################
rule run_diamond_blastp:
    threads: get_res("run_diamond_blastp", "threads")
    resources:
        mem_mb = get_res("run_diamond_blastp", "mem_mb"),
        time = get_res("run_diamond_blastp", "time")
    input:
        query = lambda wildcards: samples[wildcards.sample]
    output:
        blast_result = os.path.join(OUTPUT_BASE_DIR, "{sample}", "blast_results.m8")
    params:
        blast_db = BLAST_DB,
        blast_hits = BLAST_HITS
    shell:
        """
        mkdir -p $(dirname {output.blast_result})
        diamond blastp \
            -d {params.blast_db}.dmnd \
            -q {input.query} \
            -o {output.blast_result} \
            -p {threads} \
            -k {params.blast_hits} \
            --outfmt 6
        """

###################################
# Rule: process_blast_results
###################################
rule process_blast_results:
    threads: 1
    input:
        blast_result = os.path.join(OUTPUT_BASE_DIR, "{sample}", "blast_results.m8")
    output:
        unique_subjects = os.path.join(OUTPUT_BASE_DIR, "{sample}", "unique_subjects.txt")
    params:
        max_hits = BLAST_HITS,  # Use same limit as BLAST search
        min_hits = 5  # Minimum hits per query for warnings
    shell:
        """
        mkdir -p $(dirname {output.unique_subjects})
        python {SCRIPTS_DIR}/process_blast_for_extraction.py \
            {input.blast_result} \
            {output.unique_subjects} \
            --max-hits {params.max_hits} \
            --min-hits {params.min_hits}
        """

###################################
# Rule: check_blast_output
###################################
rule check_blast_output:
    threads: 1
    input:
        blast_result = os.path.join(OUTPUT_BASE_DIR, "{sample}", "unique_subjects.txt")
    output:
        check = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done")
    log:
        log_file = os.path.join("log", "{sample}_check_blast_output.log")
    shell:
        """
        mkdir -p $(dirname {output.check})
        mkdir -p log
        # Run the script with an absolute path
        python {SCRIPTS_DIR}/check_blast_output.py {input.blast_result} > {log.log_file} 2>&1
        # Create the done file only if the check succeeds
        touch {output.check}
        """

###################################
# Rule: extract_hits
###################################
rule extract_hits:
    threads: get_res("extract_hits", "threads")
    resources:
        mem_mb = get_res("extract_hits", "mem_mb"),
        time = get_res("extract_hits", "time")
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        unique_subjects = os.path.join(OUTPUT_BASE_DIR, "{sample}", "unique_subjects.txt")
    output:
        sequences = os.path.join(OUTPUT_BASE_DIR, "{sample}", "extracted_hits.faa"),
        error_log = os.path.join("log", "{sample}_extract_hits_errors.log")
    params:
        blast_db = BLAST_DB
    shell:
        """
        mkdir -p log
        blastdbcmd \
            -db {params.blast_db} \
            -entry_batch {input.unique_subjects} \
            > {output.sequences} 2> {output.error_log} || true
        """

###################################
# Rule: combine_sequences
###################################
rule combine_sequences:
    threads: 1
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        query = lambda wildcards: samples[wildcards.sample],
        hits = os.path.join(OUTPUT_BASE_DIR, "{sample}", "extracted_hits.faa")
    output:
        combined_sequences = os.path.join(OUTPUT_BASE_DIR, "{sample}", "combined_sequences.faa")
    log:
        os.path.join("log", "{sample}_combine_sequences.log")
    shell:
        """
        mkdir -p log
        python {SCRIPTS_DIR}/combine_and_deduplicate.py \
            {input.query} \
            {input.hits} \
            {output.combined_sequences} \
            --deduplicate-by-sequence \
            2> {log}
        """

###################################
# Rule: align_sequences
###################################
rule align_sequences:
    threads: get_res("align_sequences", "threads")
    resources:
        mem_mb = get_res("align_sequences", "mem_mb"),
        time = get_res("align_sequences", "time"),
        disk_mb = get_res("align_sequences", "disk_mb")
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        combined_sequences = os.path.join(OUTPUT_BASE_DIR, "{sample}", "combined_sequences.faa")
    output:
        aligned_sequences = os.path.join(OUTPUT_BASE_DIR, "{sample}", "aln", "aligned_sequences.msa")
    shell:
        """
        mkdir -p $(dirname {output.aligned_sequences})
        mafft --thread {threads} {input.combined_sequences} > {output.aligned_sequences}
        """

###################################
# Rule: trim_alignment
###################################
rule trim_alignment:
    threads: 1
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        aligned_sequences = os.path.join(OUTPUT_BASE_DIR, "{sample}", "aln", "aligned_sequences.msa")
    output:
        trimmed_alignment = os.path.join(OUTPUT_BASE_DIR, "{sample}", "aln", "trimmed_alignment.msa")
    shell:
        """
        trimal -in {input.aligned_sequences} -out {output.trimmed_alignment} -gt 0.1
        """

###################################
# Rule: build_tree
###################################
rule build_tree:
    threads: get_res("build_tree", "threads")
    resources:
        mem_mb = get_res("build_tree", "mem_mb"),
        time = get_res("build_tree", "time")
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        alignment = os.path.join(OUTPUT_BASE_DIR, "{sample}", "aln", "trimmed_alignment.msa")
    output:
        tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.treefile"),
        log = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.iqtree")
    params:
        outdir = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree"),
        prefix = "final_tree"
    shell:
        """
        mkdir -p {params.outdir}
        iqtree -s {input.alignment} -m LG+G4 -T {threads} --prefix {params.outdir}/{params.prefix}
        """

###################################
# Rule: extract_closest_neighbors
###################################
rule extract_closest_neighbors:
    threads: 1
    input:
        query = lambda wildcards: samples[wildcards.sample],
        unique_subjects = os.path.join(OUTPUT_BASE_DIR, "{sample}", "unique_subjects.txt"),
        tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.treefile")
    output:
        closest_neighbors = os.path.join(OUTPUT_BASE_DIR, "{sample}", "closest_neighbors.csv")
    params:
        num_neighbors = CLOSEST_NEIGHBORS,
        query_filter_cmd = lambda wildcards: f"--query_filter {QUERY_FILTER}" if QUERY_FILTER else ""
    shell:
        """
        # Extract closest neighbors from the tree
        python {SCRIPTS_DIR}/extract_closest_neighbors.py \
            --tree {input.tree} \
            --query {input.query} \
            --subjects {input.unique_subjects} \
            --output {output.closest_neighbors} \
            --num_neighbors {params.num_neighbors} \
            {params.query_filter_cmd}
        """
        
###################################
# Rule: assign_taxonomy
###################################
rule assign_taxonomy:
    threads: 1
    input:
        closest_neighbors = os.path.join(OUTPUT_BASE_DIR, "{sample}", "closest_neighbors.csv"),
        previous = lambda wildcards: (
            os.path.join(
                OUTPUT_BASE_DIR,
                sample_order[sample_order.index(wildcards.sample) - 1],
                "taxonomy_assignments.txt"
            )
            if sample_order.index(wildcards.sample) > 0 else []
        )
    output:
        taxonomy = os.path.join(OUTPUT_BASE_DIR, "{sample}", "taxonomy_assignments.txt"),
        updated_csv = os.path.join(OUTPUT_BASE_DIR, "{sample}", "closest_neighbors_with_taxonomy.csv")
    log:
        os.path.join("log", "{sample}_taxonomy_assignment.log")
    shell:
        """
        # Create log directory if it doesn't exist
        mkdir -p log
        
        # Process the closest_neighbors.csv file to add taxonomy directly
        python {SCRIPTS_DIR}/parse_closest_neighbors.py \
            -d {OUTPUT_BASE_DIR}/{wildcards.sample} \
            -o {output.taxonomy} \
            > {log} 2>&1
        """

###################################
# Rule: decorate_tree
###################################
rule decorate_tree:
    threads: 1
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.treefile"),
        taxonomy = os.path.join(OUTPUT_BASE_DIR, "{sample}", "taxonomy_assignments.txt"),
        taxonomy_csv = os.path.join(OUTPUT_BASE_DIR, "{sample}", "closest_neighbors_with_taxonomy.csv"),
        query = lambda wildcards: samples[wildcards.sample]
    output:
        decorated_tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "decorated_tree.png")
    params:
        itol_labels = os.path.join(OUTPUT_BASE_DIR, "{sample}", "itol"),
        tax_level = ITOL_TAX_LEVEL
    log:
        os.path.join("log", "{sample}_decorate_tree.log")
    shell:
        """
        python {SCRIPTS_DIR}/decorate_tree.py \
            {input.tree} {input.taxonomy} {input.query} {output.decorated_tree} {params.itol_labels} {params.tax_level} \
            2> {log}
        """

###################################
# Rule: calculate_tree_stats
###################################
rule calculate_tree_stats:
    threads: 1
    input:
        check_blast = os.path.join(OUTPUT_BASE_DIR, "{sample}", "check_blast_output.done"),
        tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.treefile"),
        taxonomy = os.path.join(OUTPUT_BASE_DIR, "{sample}", "taxonomy_assignments.txt"),
        taxonomy_csv = os.path.join(OUTPUT_BASE_DIR, "{sample}", "closest_neighbors_with_taxonomy.csv"),
        query = os.path.join(OUTPUT_BASE_DIR, "{sample}", "combined_sequences.faa")
    output:
        os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "tree_stats.tab")
    log:
        os.path.join("log", "{sample}_tree_stats.log")
    shell:
        """
        python {SCRIPTS_DIR}/tree_stats.py \
            {input.tree} {input.taxonomy} {input.query} {output} \
            2> {log}
        """

###################################
# Rule: extract_phylogenetic_placement
###################################
rule extract_phylogenetic_placement:
    threads: 1
    input:
        tree = os.path.join(OUTPUT_BASE_DIR, "{sample}", "tree", "final_tree.treefile")
    output:
        json_out = os.path.join(OUTPUT_BASE_DIR, "{sample}", "placement_results.json"),
        csv_out = os.path.join(OUTPUT_BASE_DIR, "{sample}", "placement_results.csv")
    params:
        query_prefixes = "Hype,Klos",  # Can be configured
        num_neighbors = 5,
        self_hit_threshold = 0.001
    log:
        os.path.join("log", "{sample}_phylogenetic_placement.log")
    shell:
        """
        python {SCRIPTS_DIR}/extract_phylogenetic_neighbors.py \
            --tree {input.tree} \
            --query-prefixes {params.query_prefixes} \
            --output-json {output.json_out} \
            --output-csv {output.csv_out} \
            --num-neighbors {params.num_neighbors} \
            --self-hit-threshold {params.self_hit_threshold} \
            2> {log}
        """

###################################
# Rule: combine_placement_results
###################################
rule combine_placement_results:
    threads: 1
    input:
        placement_files = expand(
            os.path.join(OUTPUT_BASE_DIR, "{sample}", "placement_results.json"),
            sample=samples.keys()
        )
    output:
        combined = os.path.join(OUTPUT_BASE_DIR, "combined_placement_results.json")
    run:
        import json

        combined_results = {
            "orthogroups": [],
            "total_queries": 0,
            "overall_taxonomy_summary": {
                "domains": {},
                "total_neighbors": 0
            }
        }

        for placement_file in input.placement_files:
            # Extract orthogroup name from path
            og_name = os.path.basename(os.path.dirname(placement_file))

            with open(placement_file, 'r') as f:
                data = json.load(f)

            # Add orthogroup data
            combined_results["orthogroups"].append({
                "name": og_name,
                "query_count": data.get("query_count", 0),
                "taxonomy_summary": data.get("taxonomy_summary", {}),
                "placements": data.get("placements", [])
            })

            # Update totals
            combined_results["total_queries"] += data.get("query_count", 0)

            # Aggregate taxonomy counts
            if "taxonomy_summary" in data and "domains" in data["taxonomy_summary"]:
                for domain, count in data["taxonomy_summary"]["domains"].items():
                    combined_results["overall_taxonomy_summary"]["domains"][domain] = \
                        combined_results["overall_taxonomy_summary"]["domains"].get(domain, 0) + count
                    combined_results["overall_taxonomy_summary"]["total_neighbors"] += count

        # Write combined results
        with open(output.combined, 'w') as f:
            json.dump(combined_results, f, indent=2)